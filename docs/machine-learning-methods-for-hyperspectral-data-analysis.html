<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Machine learning methods for hyperspectral data analysis – Remote Sensing of Foliar Nitrogen in Californian Almonds</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./radiative-transfer-models.html" rel="next">
<link href="./vegetation-indices-based-methods.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a01a0c2d2b11ceb43c7631533aab2727.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="https://damianoswald.com/index.html" class="navbar-brand navbar-brand-logo">
    <img src="./resources/iconDO.png" alt="" class="navbar-logo">
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/damian-oswald/master-thesis/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Remote-Sensing-of-Foliar-Nitrogen-in-Californian-Almonds.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./machine-learning-methods-for-hyperspectral-data-analysis.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine learning methods for hyperspectral data analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vegetation-indices-based-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vegetation-indices-based methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machine-learning-methods-for-hyperspectral-data-analysis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine learning methods for hyperspectral data analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./radiative-transfer-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Radiative transfer models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./describing-processing-and-inspecting-the-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Describing, processing and inspecting the data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./technical-description-of-the-modelling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Technical description of the modelling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./results-of-the-model-performances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Results of the model performances</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-stepwise" id="toc-sec-stepwise" class="nav-link active" data-scroll-target="#sec-stepwise"><span class="header-section-number">3.1</span> Ordinary least squares and stepwise selection</a></li>
  <li><a href="#sec-regularization" id="toc-sec-regularization" class="nav-link" data-scroll-target="#sec-regularization"><span class="header-section-number">3.2</span> Regularization</a></li>
  <li><a href="#sec-dimensionalityreduction" id="toc-sec-dimensionalityreduction" class="nav-link" data-scroll-target="#sec-dimensionalityreduction"><span class="header-section-number">3.3</span> Dimensionality reduction regression</a></li>
  <li><a href="#sec-decisiontrees" id="toc-sec-decisiontrees" class="nav-link" data-scroll-target="#sec-decisiontrees"><span class="header-section-number">3.4</span> Decision-tree-based learning</a></li>
  <li><a href="#sec-deeplearning" id="toc-sec-deeplearning" class="nav-link" data-scroll-target="#sec-deeplearning"><span class="header-section-number">3.5</span> Deep learning</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/damian-oswald/master-thesis/blob/main/machine-learning-methods-for-hyperspectral-data-analysis.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/damian-oswald/master-thesis/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ML" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine learning methods for hyperspectral data analysis</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The term machine learning was first used by <span class="citation" data-cites="samuel1959machine">Samuel (<a href="references.html#ref-samuel1959machine" role="doc-biblioref">1959</a>)</span>. He described it as the “field of study that gives computers the ability to learn without being explicitly programmed”. Back then, only few people were interested in this particular field. Today, however, this could not be further from the truth: A quick <span class="smallcaps">Scopus</span> search reveals, that in 2022 alone, 18’464 scientific publications use the term “machine learning” in their title — that is more than 50 publications a day!<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The <span class="smallcaps">Scopus</span> search for <code>machine AND learning</code> was conducted on March 7, 2023. Only the document type “article” was searched for. 449’663 publications of all time contain “machine learning” in their title, abstract or keywords. 57’926 contain “machine learning” in the title only. Out of all the articles ever using “machine learning” in their title, 32% were published in 2022.</p></div></div><p>The substantial advancements witnessed in machine learning over the past two decades can be largely ascribed to the surge in computational power, the sudden availability of copious amounts of data, and the open-source culture that enables applications through readily accessible machine learning libraries and frameworks <span class="citation" data-cites="jordan2015machine fridman2019">(<a href="references.html#ref-jordan2015machine" role="doc-biblioref">Jordan and Mitchell 2015</a>, <a href="references.html#ref-fridman2019" role="doc-biblioref">Fridman 2019</a>)</span>.</p>
<p>In this thesis, I will use the term “machine learning” as originally defined by Arthur Samuel. As per his definition, machine learning includes a variety of mathematical models from chemometrics and statistics, despite the fact that many of these models were developed prior to the coining of the term itself. The essential aspect of these models lies in their primary function for making predictions, rather than serving as statistical models. Moreover, the machine learning models evaluated in this thesis are differentiated from hand-crafted features such as vegetation indices and radiative transfer models by their fully automatic process of selecting and combining spectral bands.</p>
<p>Owing to their exceptional predictive capabilities and handling of high-dimensional input data, machine learning algorithms have emerged as a widely adopted tool for modern hyperspectral image analysis <span class="citation" data-cites="gewali2018machine">(<a href="references.html#ref-gewali2018machine" role="doc-biblioref">Gewali et al. 2018</a>)</span>. In this chapter, various machine learning algorithms that have been extensively used on hyperspectral data will be examined. To do this, I will loosely follow the route taken by <span class="citation" data-cites="james2013introduction">James et al. (<a href="references.html#ref-james2013introduction" role="doc-biblioref">2013</a>)</span>, commencing with the ordinary least squares model and addressing why it is a poor choice for hyperspectral data, and subsequently slowly relaxing the assumptions of the linear model.</p>
<section id="sec-stepwise" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-stepwise"><span class="header-section-number">3.1</span> Ordinary least squares and stepwise selection</h2>
<p>The simplest and also probably most prevalently used machine learning model is the linear regression model <span class="citation" data-cites="yan2009linear">(<a href="references.html#ref-yan2009linear" role="doc-biblioref">Yan and Su 2009</a>)</span>. In its univariate form, it can be represented as follows.</p>
<p><span id="eq-generalLinearModel"><span class="math display">\[\mathbf y = \mathbf X \boldsymbol \beta + \boldsymbol \varepsilon \tag{3.1}\]</span></span></p>
<p>Here, <span class="math inline">\(\mathbf y\)</span> represents the labels, <span class="math inline">\(\mathbf X\)</span> denotes the design matrix comprising all predictors, <span class="math inline">\(\boldsymbol \beta\)</span> signifies the coefficients, and <span class="math inline">\(\boldsymbol \varepsilon\)</span> corresponds to the model’s errors. The customary approach for estimating the coefficients <span class="math inline">\({\boldsymbol \beta}\)</span> in <a href="#eq-generalLinearModel" class="quarto-xref">equation&nbsp;<span>3.1</span></a> employs the analytical solution using the ordinary least squares estimator.</p>
<p><span id="eq-ols"><span class="math display">\[\hat {\boldsymbol \beta} = (\mathbf X^\top\mathbf X)^{-1} \mathbf X^\top \mathbf y \tag{3.2}\]</span></span></p>
<p>This method was (allegedly) introduced independently by <span class="citation" data-cites="legendre1806nouvelles">Legendre (<a href="references.html#ref-legendre1806nouvelles" role="doc-biblioref">1806</a>)</span> in <em>Nouvelles méthodes pour la détermination des orbites des comètes</em> (new methods for the determination of comet orbits) and <span class="citation" data-cites="gauss1809theoria">Gauss (<a href="references.html#ref-gauss1809theoria" role="doc-biblioref">1809</a>)</span> in <em>Theoria Motus Corporum Coelestium</em> (theory of the motion of the heavenly bodies), eventually being formalized at a later date by <span class="citation" data-cites="markov1912wahrscheinlichkeitsrechnung">Markov (<a href="references.html#ref-markov1912wahrscheinlichkeitsrechnung" role="doc-biblioref">1912</a>)</span>. However, it is worth noting that Markov did not contribute any novel proof <span class="citation" data-cites="plackett1949historical yan2009linear">(<a href="references.html#ref-plackett1949historical" role="doc-biblioref">Plackett 1949</a>, <a href="references.html#ref-yan2009linear" role="doc-biblioref">Yan and Su 2009</a>)</span>.</p>
<p>In what is now known as the Gauss-Markov theorem, Markov asserts that the ordinary least squares estimator is the best linear unbiased estimator under the specific conditions that the error’s expectation is zero (<span class="math inline">\(\mathbb E(\varepsilon_i) = 0\)</span>), the error exhibits equal variance (<span class="math inline">\(\text{Var}(\varepsilon_i) = \sigma^2\)</span>), and all distinct error terms are uncorrelated (<span class="math inline">\(\text{Cov}(\varepsilon_i, \varepsilon_j) = 0\)</span>).</p>
<p>Unprocessed hyperspectral reflectance data frequently violates the assumptions of the Gauss-Markov theorem — particularly the third assumption regarding uncorrelated errors. While equal variance can be achieved through z-score normalization (a technique described in more detail in <a href="describing-processing-and-inspecting-the-data.html#sec-normalization" class="quarto-xref">section&nbsp;<span>5.4</span></a>), and the expectation of zero error might be satisfied if the response is investigated on a small enough scale, the assumption of uncorrelated error terms cannot be fulfilled while retaining all available variables or information about all bands. The crux of the problem lies in so-called multicollinearity: the reflectance of neighbouring spectral bands is typically very strongly correlated. Consequently, the reflectance at one band can be predicted linearly from others with a very high precision <span class="citation" data-cites="chlingaryan2018machine">(<a href="references.html#ref-chlingaryan2018machine" role="doc-biblioref">Chlingaryan et al. 2018</a>)</span>. Because of multicollinearity, minor changes in the data or model specification can have a disproportional effect on parameter estimates <span class="citation" data-cites="ravikanth2017extraction">(<a href="references.html#ref-ravikanth2017extraction" role="doc-biblioref">Ravikanth et al. 2017</a>)</span>.</p>
<p>Ordinary least squares also encounter difficulties in hyperspectral data analysis due to the <em>small n, large p problem</em>. In many agricultural applications of hyperspectral data, there is often a limit to the number of samples taken, since the acquisition of observations can be both costly and labour-intensive <span class="citation" data-cites="ravikanth2017extraction">(<a href="references.html#ref-ravikanth2017extraction" role="doc-biblioref">Ravikanth et al. 2017</a>)</span>. When the design matrix <span class="math inline">\(\mathbf X\)</span> has fewer rows (observations, <span class="math inline">\(n\)</span>) than columns (parameters, <span class="math inline">\(p\)</span>), the inversion of <span class="math inline">\(\mathbf X^\top\mathbf X\)</span> in <a href="#eq-ols" class="quarto-xref">equation&nbsp;<span>3.2</span></a> becomes impossible.</p>
<p>If one wanted to strictly stick to linear regression using ordinary least squares, a potential workaround evading the <em>small n, large p problem</em> would be the application of a prior feature selection strategy to tackle multicollinearity (remember, vegetation indices as discussed in the chapter before essentially constituted such a feature selection method). Provided that there are more observations than parameters to estimate, feature selection methods like stepwise selection can also further create a subset of variables for the linear regression model such that it is no longer over-specified. These methods minimize a so-called information criterion. This metric balances both the model’s likelihood and penalizes over-parametrization simultaneously. The Akaike information criterion (AIC), introduced by <span class="citation" data-cites="akaike1998information">Akaike (<a href="references.html#ref-akaike1998information" role="doc-biblioref">1998</a>)</span>, serves as one such criterion for comparing the fit of various regression models. It can be calculated as follows.</p>
<p><span id="eq-AIC"><span class="math display">\[\text{AIC} = k(p - \ln \hat {L}) \tag{3.3}\]</span></span></p>
<p>In this equation, <span class="math inline">\(p\)</span> represents the number of parameters, <span class="math inline">\(k\)</span> is a scalar, and <span class="math inline">\(\ln \hat {L}\)</span> denotes the estimated log-likelihood of the model. A true AIC is obtained when <span class="math inline">\(k=2\)</span>, while <span class="math inline">\(k=\ln(n)\)</span> (where <span class="math inline">\(n\)</span> is the number of observations) is sometimes called the Bayesian information criterion (BIC) or Schwarz information criterion (SIC). Through stepwise selection, variables are iteratively added to and removed from the model so as to minimize the information criterion.</p>
<p>A practical application of this approach can be seen in the research conducted by <span class="citation" data-cites="castaldi2016data">Castaldi et al. (<a href="references.html#ref-castaldi2016data" role="doc-biblioref">2016</a>)</span>. The authors focused on predicting wheat grain nitrogen uptake from satellite data. To identify a suitable subset of spectral bands, they executed a stepwise selection process on the variables of a multiple linear regression model.</p>
<p>However, the application of stepwise selection to hyperspectral data has faced substantial criticism <span class="citation" data-cites="bolster1996determination grossman1996critique">(<a href="references.html#ref-bolster1996determination" role="doc-biblioref">Bolster et al. 1996</a>, <a href="references.html#ref-grossman1996critique" role="doc-biblioref">Grossman et al. 1996</a>)</span>. The key issue is that its performance markedly declines as the number of utilized bands increases. When dealing with a too large number of variables, stepwise selection begins to select a seemingly random subset of bands for linear regression, which subsequently results in overfitting <span class="citation" data-cites="hastie2017extended">(<a href="references.html#ref-hastie2017extended" role="doc-biblioref">Hastie et al. 2017</a>)</span>. In situations where a linear regression model incorporating more than a handful of bands is sought, regularization of the model may prove to be a more suitable solution.</p>
</section>
<section id="sec-regularization" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-regularization"><span class="header-section-number">3.2</span> Regularization</h2>
<p>Regularization is a technique extensively employed in machine learning in order to avoid overfitting and enhance the generalization of models. Applicable to linear models as well as more intricate ones, such as neural networks or decision-tree-based models, regularization operates by adding a penalty term to the original, unregularized cost function <span class="math inline">\(J\)</span>, which in turn penalizes the magnitude of the estimated coefficient. Consequently, larger coefficient estimates lead to a greater penalty <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. This process forces the regression model to strike a balance between small (or few) coefficients and an effective explanation of the data. In that sense, regularization can be understood as Occam’s Razor<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> applied to machine learning models, favouring simpler models over their more complex counterparts — even if the more complex ones could better explain the data. The underlying reasoning for this approach is that, when given enough flexibility, models inherently tend to over-explain the data. As a countermeasure, regularization restricts some of that flexibility, promoting a more balanced model.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Occam’s razor is a philosophical principle that suggests that, among competing hypotheses, the simplest one with the fewest assumptions is preferable or more likely to be true <span class="citation" data-cites="van2018occam">(<a href="references.html#ref-van2018occam" role="doc-biblioref">Van den Berg 2018</a>)</span>.</p></div></div><p>The penalty term is typically based on various vector norms of the model coefficients, such as the <span class="math inline">\(\ell^1\)</span>-norm (referred to as lasso regularization) or the <span class="math inline">\(\ell^2\)</span>-norm (known as ridge regularization). The cost function <span class="math inline">\(J\)</span> for both lasso and ridge regression can be expressed in a single equation as follows.</p>
<p><span id="eq-RLSloss"><span class="math display">\[J(\boldsymbol \beta, \mathbf X, \mathbf y) =
\| \mathbf {y -X\boldsymbol\beta} \|_2^2 +
\lambda \left[ \frac{1}{2}(1-\alpha)\| \boldsymbol \beta \|_2^2 + \alpha \| \boldsymbol \beta \| \right] \tag{3.4}\]</span></span></p>
<p>Here, <span class="math inline">\(\| \cdot \|\)</span> represents the absolute value norm or the <span class="math inline">\(\ell^1\)</span>-norm, while <span class="math inline">\(\| \cdot \|_2\)</span> denotes the Euclidean norm or the <span class="math inline">\(\ell^2\)</span>-norm. It is worth noting that the first term in <a href="#eq-RLSloss" class="quarto-xref">equation&nbsp;<span>3.4</span></a> (<span class="math inline">\(\| \mathbf {y -X\boldsymbol\beta} \|_2^2\)</span>) is simply the standard cost function for linear least squares regression, as it equates to the sum of squared errors. The second half constitutes the penalty term that regularizes the coefficients. The parameter <span class="math inline">\(\lambda\)</span> determines the magnitude of the penalty, as it is multiplied by a mixture of norms of the coefficient vector <span class="math inline">\(\boldsymbol \beta\)</span>. If one exclusively selects <span class="math inline">\(\alpha = 0\)</span>, that results in ridge regression, since only the <span class="math inline">\(\ell^2\)</span>-norm is employed. Conversely, choosing <span class="math inline">\(\alpha = 1\)</span> relies solely on the <span class="math inline">\(\ell^1\)</span>-norm, resulting in lasso regression. Any value between <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> yields a combination of the two, referred to as the elastic net <span class="citation" data-cites="zou2005regularization james2013introduction">(<a href="references.html#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>, <a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. Consequently, we can regard <span class="math inline">\(\lambda \in [0, \infty)\)</span> as a parameter controlling the penalty’s magnitude and <span class="math inline">\(\alpha \in [0,1]\)</span> as a parameter directing the type of regularization applied.</p>
<p>In general, regularization aids in mitigating multicollinearity issues, consequently reducing the variance of coefficient estimates. Ridge regression, based on the Euclidean norm, progressively decreases the penalty on coefficient estimates as they approach zero. This means, that for ridge regression, the variables in the model will always exert an effect, albeit a tiny one in the case of useless variables.</p>
<p>Lasso regression on the other hand, initially introduced by <span class="citation" data-cites="tibshirani1996">Tibshirani (<a href="references.html#ref-tibshirani1996" role="doc-biblioref">1996</a>)</span>, begins to induce sparsity in the data matrix as <span class="math inline">\(\lambda\)</span> increases <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. This property means that lasso regression leads to a selection of variables that effectively explain the response. Unlike stepwise selection, lasso regression does not suffer from a high number of simultaneously used variables <span class="citation" data-cites="hastie2017extended">(<a href="references.html#ref-hastie2017extended" role="doc-biblioref">Hastie et al. 2017</a>)</span>. Lasso regression emerges as a particularly appealing tool for hyperspectral data analysis, as it facilitates the selection of a limited number of bands that best explain the response variable <span class="citation" data-cites="takayama2016optimal">(<a href="references.html#ref-takayama2016optimal" role="doc-biblioref">Takayama and Iwasaki 2016</a>)</span>.</p>
<p>The number of selected bands can be effectively controlled via <span class="math inline">\(\lambda\)</span>. Lasso regression has already been employed for spectral band selection in spectroradiometric leaf-level data, specifically in the context of foliar nitrogen prediction in vineyards <span class="citation" data-cites="omidi2020ensemble">(<a href="references.html#ref-omidi2020ensemble" role="doc-biblioref">Omidi et al. 2020</a>)</span>.</p>
</section>
<section id="sec-dimensionalityreduction" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-dimensionalityreduction"><span class="header-section-number">3.3</span> Dimensionality reduction regression</h2>
<p>Thus far, the focus has been on regression methods that address multicollinearity and the small <span class="math inline">\(n\)</span>, large <span class="math inline">\(p\)</span> problem by selecting features, penalizing coefficients, or both, as in the case of lasso regression. An alternative approach to tackling these issues involves formulating the regression after applying a dimensionality reduction technique. Principal component regression (PCR) and partial least squares regression (PLSR) stand out as the two most prominent methods for achieving this. Partial least squares, originally introduced by <span class="citation" data-cites="wold1984collinearity">Wold et al. (<a href="references.html#ref-wold1984collinearity" role="doc-biblioref">1984</a>)</span>, is particularly prevalent in chemometrics, even though it was initially introduction as an econometric tool for addressing multicollinearity <span class="citation" data-cites="geladi1986partial mehmood2016diversity">(<a href="references.html#ref-geladi1986partial" role="doc-biblioref">Geladi and Kowalski 1986</a>, <a href="references.html#ref-mehmood2016diversity" role="doc-biblioref">Mehmood and Ahmed 2016</a>)</span>.</p>
<p>The two related methods can be mathematically expressed in a similar manner. Any design matrix <span class="math inline">\(\mathbf X\)</span> containing the original data can be represented by its score matrix <span class="math inline">\(\mathbf T\)</span>.</p>
<p><span id="eq-pca"><span class="math display">\[\mathbf T = \mathbf{XW} \tag{3.5}\]</span></span></p>
<p>Here, <span class="math inline">\(\mathbf W\)</span> is a <span class="math inline">\(p \times p\)</span> rotation matrix of weights. In the case of principal component analysis (PCA), the columns of <span class="math inline">\(\mathbf W\)</span> are the eigenvectors of <span class="math inline">\(\text{Cov}(\mathbf X)\)</span>. The full matrix <span class="math inline">\(\mathbf T\)</span> is thus a linear re-combination of <span class="math inline">\(\mathbf X\)</span>, but it conveniently contains maximal information in its first column, with the remaining information successively distributed in subsequent columns. The constructed variables in the score matrix, called principal components, are completely uncorrelated with each other. These principal components, also referred to as latent variables, are not directly measured but are constructed from the observed variables.</p>
<p>The lack of multicollinearity among the latent variables in the score matrix makes them better suited for ordinary least squares regression than the observed data. The more collinear the original data, the fewer principal components are needed to capture the essence of the data. This effectively eliminates the small <span class="math inline">\(n\)</span>, large <span class="math inline">\(p\)</span> problem. After performing linear dimensionality reduction on <span class="math inline">\(\mathbf X\)</span>, linear regression can be applied to a limited number of variables in the score matrix <span class="math inline">\(\mathbf T\)</span> to avoid overfitting issues typically associated with stepwise selection <span class="citation" data-cites="hansen2003reflectance grossman1996critique">(<a href="references.html#ref-grossman1996critique" role="doc-biblioref">Grossman et al. 1996</a>, <a href="references.html#ref-hansen2003reflectance" role="doc-biblioref">Hansen and Schjoerring 2003</a>)</span>. Both principal component and partial least squares regression can thus be formulated as follows.</p>
<p><span id="eq-pcr"><span class="math display">\[\mathbf y = \mathbf T_{[1,l]} \: \boldsymbol \beta + \boldsymbol \varepsilon \tag{3.6}\]</span></span></p>
<p>Here, <span class="math inline">\(l\)</span> is the number of variables from <span class="math inline">\(\mathbf T\)</span> used for the regression, and the rest is analogous to <a href="#eq-generalLinearModel" class="quarto-xref">equation&nbsp;<span>3.1</span></a>. The difference between principal component and partial least square regression lies in how the score matrix <span class="math inline">\(\mathbf T\)</span> and the rotation matrix <span class="math inline">\(\mathbf W\)</span> are estimated. In principal component regression, a principal component analysis is performed without considering the response <span class="math inline">\(\mathbf y\)</span>, but instead maximizing the variance in the columns of the score matrix <span class="math inline">\(\mathbf T\)</span> successively. However, this strategy could theoretically find principal components which explain maximal variance in the data, but do not in fact correlate with the labels <span class="citation" data-cites="geladi1986partial">(<a href="references.html#ref-geladi1986partial" role="doc-biblioref">Geladi and Kowalski 1986</a>)</span>.</p>
<p>Partial least squares regression, introduced by <span class="citation" data-cites="wold1984collinearity">Wold et al. (<a href="references.html#ref-wold1984collinearity" role="doc-biblioref">1984</a>)</span>, addresses this issue by informing the estimation of the rotation matrix with both the labels and the data at the same time, thus maximizing the predictive power of the latent variables. Specifically, partial least squares estimate <span class="math inline">\(\mathbf W\)</span> to maximize the covariance between <span class="math inline">\(\mathbf y\)</span> and <span class="math inline">\(\mathbf T\)</span>:</p>
<p><span id="eq-plsr"><span class="math display">\[\mathbf W = \arg \max_{\mathbf W} \left\{ \text{Cov} \left( \mathbf y, \mathbf {XW} \right) \right\} \qquad\text{s.t. } \mathbf W ^\top \mathbf W = \mathbf I \tag{3.7}\]</span></span></p>
<p>In this equation, <span class="math inline">\(\mathbf I\)</span> represents the identity matrix <span class="citation" data-cites="bennett2003optimization">(<a href="references.html#ref-bennett2003optimization" role="doc-biblioref">Bennett and Embrechts 2003</a>)</span>. Note that <a href="#eq-plsr" class="quarto-xref">equation&nbsp;<span>3.7</span></a> describes the maximization task for univariate partial least squares, applicable to cases with a single response variable, although the model can also be applied to multivariate cases, where a rotation matrix is found to maximize the covariance of the principal components with respect to multiple responses simultaneously.</p>
<p>In summary, principal component regression maximizes the variance of the principal components with respect to <span class="math inline">\(\mathbf X\)</span> and subsequently performs a regression on <span class="math inline">\(\mathbf y\)</span>. In contrast, partial least squares regression maximizes the covariance between the components <span class="math inline">\(\mathbf T\)</span> and the independent variable <span class="math inline">\(\mathbf y\)</span>. Therefore, if the primary goal is to make predictions using linear regression analysis, partial least squares regression is generally a better alternative compared to principal component regression <span class="citation" data-cites="inoue2012diagnostic liu2022partial">(<a href="references.html#ref-inoue2012diagnostic" role="doc-biblioref">Inoue et al. 2012</a>, <a href="references.html#ref-liu2022partial" role="doc-biblioref">Liu et al. 2022</a>)</span>.</p>
<p>Due to these properties, partial least squares regression is an excellent method for predicting a response based on hyperspectral data. It is also the most widely tested model for predicting crop nitrogen <span class="citation" data-cites="berger2020crop">(<a href="references.html#ref-berger2020crop" role="doc-biblioref">Berger et al. 2020</a>)</span>. For instance, partial least squares were successfully used by <span class="citation" data-cites="wang2017non">Wang et al. (<a href="references.html#ref-wang2017non" role="doc-biblioref">2017</a>)</span> to predict nitrogen in leaves from pear trees, with a validation R<sup>2</sup> value of 0.85.</p>
<p>However, there are two major weaknesses to the partial least squares approach when it comes to hyperspectral data that must be considered. First, the regression method utilizes the entire spectrum of the original variables to construct the latent variables. While this can provide some insights into which bands are more explanatory for the response, it is not as straightforward as other methods for selecting a minimum number of relevant bands that can adequately explain the response. Band selection via partial least squares requires additional analysis of the model <span class="citation" data-cites="mehmood2016diversity">(<a href="references.html#ref-mehmood2016diversity" role="doc-biblioref">Mehmood and Ahmed 2016</a>)</span>. Second, partial least squares regression is inherently linear in nature. As a result, despite its powerful capabilities in modelling linear relationships, it may fail to capture complex, non-linear relationships present in the data — relationships that are known to exist between the foliar nitrogen concentration and the leaf’s reflectance.</p>
<p>When the underlying structure of the data is strongly non-linear, principal component and partial least squares regression may not produce satisfactory results. In such cases, their kernelized counterparts can be explored, which allow for non-linear dimensionality reduction before the regression stage <span class="citation" data-cites="rosipal2006overview">(<a href="references.html#ref-rosipal2006overview" role="doc-biblioref">Rosipal and Krämer 2006</a>)</span>. Alternatively, other machine learning techniques, such as decision-tree-based models, Gaussian process regression, support vector regression, or artificial neural networks, might be more appropriate for modelling intrinsically non-linear relationships.</p>
</section>
<section id="sec-decisiontrees" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-decisiontrees"><span class="header-section-number">3.4</span> Decision-tree-based learning</h2>
<p>Decision-tree-based methods are vastly different from the machine learning algorithms examined thus far. A decision tree can be constructed by recursively partitioning the data into subsets based on the values of individual features, and then establishing individual rules for the created subsets. The paths in these trees are called branches, and the terminal nodes are called leaves <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. Decision trees can be employed for both classification and regression; however, since the focus of this thesis is the prediction of foliar nitrogen concentration, the discussion will centre on regression trees. Decision-tree-based learning has been widely utilized in various applications, including remote sensing applications for nitrogen detection <span class="citation" data-cites="chlingaryan2018machine">(<a href="references.html#ref-chlingaryan2018machine" role="doc-biblioref">Chlingaryan et al. 2018</a>)</span>.</p>
<p>When used individually, decision trees often either overfit the data or fail to effectively approximate the underlying function. However, their power is significantly enhanced when employed in ensemble learning, which combines multiple trees to improve performance. Two popular ensemble methods include bagging, where trees are trained on randomly selected subsets of the data, and boosting, where trees are iteratively trained to correct the errors of their predecessors <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>.</p>
<p>One of the most prevalent bagging methods is random forests, a model introduced by <span class="citation" data-cites="breiman2001">Breiman (<a href="references.html#ref-breiman2001" role="doc-biblioref">2001</a>)</span>. Each tree within a random forest is trained on only a subset of the observations and a limited number of variables, which allows less informative variables to influence the outcome as well. Predictions made by a random forest are simply the mean of all individual predictions of decision trees <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. Random forests have been employed by <span class="citation" data-cites="jin2020advancing">Jin et al. (<a href="references.html#ref-jin2020advancing" role="doc-biblioref">2020</a>)</span> due to their exceptional capability of handling non-linear relationships in order to determine which variables are essential for yield determination in Californian almonds.</p>
<p>As an alternative to bagging and random forests, the technique of boosting combines multiple weak learners, which are decision trees that perform only slightly better than random guessing, into a robust model capable of making accurate predictions <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. The key aspect of this approach is that weak learners are fitted sequentially, with new trees attempting to reduce the error of their predecessors. Consequently, subsequent learners focus on the aspects that previous learners failed to capture. The final model is a weighted sum of the individual weak learners, with the weights determined by each learner’s relative performance. One of the latest examples of boosting is the extreme gradient boosting (XGBoost) framework, introduced by <span class="citation" data-cites="chen2016xgboost">Chen and Guestrin (<a href="references.html#ref-chen2016xgboost" role="doc-biblioref">2016</a>)</span>. This particular algorithm has proven particularly useful for predicting the foliar nitrogen concentration of grapevines in California <span class="citation" data-cites="moghimi2020novel">(<a href="references.html#ref-moghimi2020novel" role="doc-biblioref">Moghimi et al. 2020</a>)</span>.</p>
<p>A third variation of ensemble learning with decision trees are cubist trees, which incorporate linear regression models as node rules. This adds flexibility to the model, making the response a linear function at any point — unlike the prior two models, which are essentially limited to always be step functions.</p>
<p>In general, decision-tree-based learning can be particularly effective when applied to hyperspectral data, as the trees can manage numerous dimensions and automatically select the most relevant bands at each node. They can adeptly handle non-linear relationships, a capability that some previously discussed methods lack. Additionally, their hierarchical structure is easy to understand and interpret, reflecting the decision-making process in a clear and comprehensible manner. This advantage enables drawing conclusions about the importance of specific variables for prediction, making decision-tree-based learning useful for spectral band selection, especially in classification tasks <span class="citation" data-cites="omidi2020ensemble">(<a href="references.html#ref-omidi2020ensemble" role="doc-biblioref">Omidi et al. 2020</a>)</span>.</p>
<p>Decision-tree-based methods have the notable drawback in that they tend to be considerably slower during the inference stage compared to other models. Remember that when making predictions with an ensemble method, all the trained models need to calculate the result, which can contribute to a longer computation time compared to other models. This disadvantage becomes especially apparent when predictions must be for every pixel in a hyperspectral image which may potentially require billions of predictions. Additionally, in regression, decision-tree-based learning may suffer from the fact that they generate inherently non-smooth functions. If smooth non-linear functions are required, methods such as support vector regression, Gaussian process regression or even artificial neural networks might be more suitable alternatives.</p>
</section>
<section id="sec-deeplearning" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-deeplearning"><span class="header-section-number">3.5</span> Deep learning</h2>
<p>Much of today’s remarkable advances in the field of machine learning are largely based on a group of models called artificial neural networks. These models power the most advanced artificial intelligence systems, which effortlessly outperform humans in chess, Go, or video games <span class="citation" data-cites="stockfish2023 silver2017mastering vinyals2019grandmaster">(<a href="references.html#ref-silver2017mastering" role="doc-biblioref">Silver et al. 2017</a>, <a href="references.html#ref-vinyals2019grandmaster" role="doc-biblioref">Vinyals et al. 2019</a>, <a href="references.html#ref-stockfish2023" role="doc-biblioref">Noda 2023</a>)</span>. Artificial neural networks are also applied in various scientific domains, such as chaos control in fusion reactors <span class="citation" data-cites="degrave2022magnetic">(<a href="references.html#ref-degrave2022magnetic" role="doc-biblioref">Degrave et al. 2022</a>)</span> or for highly accurate protein structure prediction <span class="citation" data-cites="jumper2021highly">(<a href="references.html#ref-jumper2021highly" role="doc-biblioref">Jumper et al. 2021</a>)</span>. In addition, artificial neural networks have made a significant impact in recent years in the form of generative artificial intelligence that can produce images, audio, or text <span class="citation" data-cites="openai2023 rombach2021highresolution">(<a href="references.html#ref-rombach2021highresolution" role="doc-biblioref">Rombach et al. 2021</a>, <a href="references.html#ref-openai2023" role="doc-biblioref">OpenAI 2023</a>)</span>. Some researchers even believe that the largest neural networks exhibit sparks of artificial general intelligence — that is artificial intelligence with human-like understanding and adaptability <span class="citation" data-cites="bubeck2023sparks">(<a href="references.html#ref-bubeck2023sparks" role="doc-biblioref">Bubeck et al. 2023</a>)</span>.</p>
<p>Artificial neural networks are a type of machine learning algorithm that is loosely inspired by the structure and function of the human brain. The concept of artificial neural networks pre-dates many of the other models discussed so far: the so-called perceptron was formally introduced by <span class="citation" data-cites="rosenblatt1958perceptron">Rosenblatt (<a href="references.html#ref-rosenblatt1958perceptron" role="doc-biblioref">1958</a>)</span>, with similar ideas dating as far back as to <span class="citation" data-cites="mcculloch1943logical">McCulloch and Pitts (<a href="references.html#ref-mcculloch1943logical" role="doc-biblioref">1943</a>)</span>. Neural networks consist of multiple layers of interconnected nodes, known as neurons. Each neuron essentially functions as a generalized linear model. These neurons are organized into input, output, and hidden layers. The stacking of multiple hidden layers was referred to as deep neural networks, which gave rise to the term deep learning — basically referring to the usage of neural networks in order to solve a task.</p>
<p>Despite their numerous benefits, neural networks have some significant drawbacks: they only tend to excel in domains with large sample sizes, extremely non-linear functions, and very high-dimensional data. While the last of these characteristics applies to hyperspectral data, the first two are rarely satisfied in this context. A suitable application of neural networks in hyperspectral data analysis would therefore be one where ample data is available, such as in the case of synthetic data from simulations. Hence, artificial neural networks are commonly applied to approximate the inverse of radiative transfer models <span class="citation" data-cites="verger2011optimal">(<a href="references.html#ref-verger2011optimal" role="doc-biblioref">Verger et al. 2011</a>)</span>.</p>
<p>Moreover, training artificial neural networks is more challenging due to the large number of tuning parameters that must be set adequately <span class="citation" data-cites="chlingaryan2018machine">(<a href="references.html#ref-chlingaryan2018machine" role="doc-biblioref">Chlingaryan et al. 2018</a>)</span>. And more than any of the previously discussed machine learning models, artificial neural networks exemplify a common issue with all data-driven methods: while they may excel at recognizing patterns and making predictions, they do not necessarily enhance human understanding of processes. This is why <span class="citation" data-cites="berger2020crop">Berger et al. (<a href="references.html#ref-berger2020crop" role="doc-biblioref">2020</a>)</span> advocates for further exploration of physics-based models, particularly those modelling radiative transfer.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-akaike1998information" class="csl-entry" role="listitem">
Akaike, H. 1998. <a href="https://doi.org/10.1007/978-1-4612-1694-0_15">Information theory and an extension of the maximum likelihood principle</a>. Selected papers of Hirotugu Akaike:199–213.
</div>
<div id="ref-bennett2003optimization" class="csl-entry" role="listitem">
Bennett, K., and M. Embrechts. 2003. An optimization perspective on kernel partial least squares regression. Nato Science Series sub series III computer and systems sciences 190:227–250.
</div>
<div id="ref-berger2020crop" class="csl-entry" role="listitem">
Berger, K., J. Verrelst, J.-B. Feret, Z. Wang, M. Wocher, M. Strathmann, M. Danner, W. Mauser, and T. Hank. 2020. <a href="https://doi.org/10.1016/j.rse.2020.111758">Crop nitrogen monitoring: Recent progress and principal developments in the context of imaging spectroscopy missions</a>. Remote Sensing of Environment 242:111758.
</div>
<div id="ref-bolster1996determination" class="csl-entry" role="listitem">
Bolster, K. L., M. E. Martin, and J. D. Aber. 1996. <a href="https://doi.org/10.1139/x26-068">Determination of carbon fraction and nitrogen concentration in tree foliage by near infrared reflectances: A comparison of statistical methods</a>. Canadian journal of forest research 26:590–600.
</div>
<div id="ref-breiman2001" class="csl-entry" role="listitem">
Breiman, L. 2001. <a href="https://doi.org/10.1023/A:1010933404324">Random forests</a>. Machine Learning 45:5–32.
</div>
<div id="ref-bubeck2023sparks" class="csl-entry" role="listitem">
Bubeck, S., V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, and others. 2023. <a href="https://doi.org/10.48550/arXiv.2303.12712">Sparks of artificial general intelligence: Early experiments with gpt-4</a>. arXiv preprint arXiv:2303.12712.
</div>
<div id="ref-castaldi2016data" class="csl-entry" role="listitem">
Castaldi, F., A. Castrignanò, and R. Casa. 2016. <a href="https://doi.org/10.1080/01431161.2016.1212423">A data fusion and spatial data analysis approach for the estimation of wheat grain nitrogen uptake from satellite data</a>. International Journal of Remote Sensing 37:4317–4336.
</div>
<div id="ref-chen2016xgboost" class="csl-entry" role="listitem">
Chen, T., and C. Guestrin. 2016. XGBoost: A scalable tree boosting system. Pages 785–794 Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining.
</div>
<div id="ref-chlingaryan2018machine" class="csl-entry" role="listitem">
Chlingaryan, A., S. Sukkarieh, and B. Whelan. 2018. <a href="https://doi.org/10.1016/j.compag.2018.05.012">Machine learning approaches for crop yield prediction and nitrogen status estimation in precision agriculture: A review</a>. Computers and Electronics in Agriculture 151:61–69.
</div>
<div id="ref-degrave2022magnetic" class="csl-entry" role="listitem">
Degrave, J., F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki, D. de Las Casas, and others. 2022. <a href="https://doi.org/10.1038/s41586-021-04301-9">Magnetic control of tokamak plasmas through deep reinforcement learning</a>. Nature 602:414–419.
</div>
<div id="ref-fridman2019" class="csl-entry" role="listitem">
Fridman, L. 2019. <a href="https://youtu.be/O5xeyoRL95U?t=115">Deep learning basics</a>. Lecture.
</div>
<div id="ref-gauss1809theoria" class="csl-entry" role="listitem">
Gauss, C. F. 1809. Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore carolo friderico gauss. Sumtibus Frid. Perthes et IH Besser.
</div>
<div id="ref-geladi1986partial" class="csl-entry" role="listitem">
Geladi, P., and B. R. Kowalski. 1986. <a href="https://doi.org/10.1016/0003-2670(86)80028-9">Partial least-squares regression: A tutorial</a>. Analytica chimica acta 185:1–17.
</div>
<div id="ref-gewali2018machine" class="csl-entry" role="listitem">
Gewali, U. B., S. T. Monteiro, and E. Saber. 2018. <a href="https://doi.org/10.48550/arXiv.1802.08701">Machine learning based hyperspectral image analysis: A survey</a>. arXiv.
</div>
<div id="ref-grossman1996critique" class="csl-entry" role="listitem">
Grossman, Y., S. Ustin, S. Jacquemoud, E. Sanderson, G. Schmuck, and J. Verdebout. 1996. <a href="https://doi.org/10.1016/0034-4257(95)00235-9">Critique of stepwise multiple linear regression for the extraction of leaf biochemistry information from leaf reflectance data</a>. Remote Sensing of Environment 56:182–193.
</div>
<div id="ref-hansen2003reflectance" class="csl-entry" role="listitem">
Hansen, P., and J. Schjoerring. 2003. <a href="https://doi.org/10.1016/S0034-4257(03)00131-7">Reflectance measurement of canopy biomass and nitrogen status in wheat crops using normalized difference vegetation indices and partial least squares regression</a>. Remote Sensing of Environment 86:542–553.
</div>
<div id="ref-hastie2017extended" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, and R. J. Tibshirani. 2017. <a href="https://doi.org/10.48550/arXiv.1707.08692">Extended comparisons of best subset selection, forward stepwise selection, and the lasso</a>. arXiv preprint arXiv:1707.08692.
</div>
<div id="ref-inoue2012diagnostic" class="csl-entry" role="listitem">
Inoue, Y., E. Sakaiya, Y. Zhu, and W. Takahashi. 2012. <a href="https://doi.org/10.1016/j.rse.2012.08.026">Diagnostic mapping of canopy nitrogen content in rice based on hyperspectral measurements</a>. Remote Sensing of Environment 126:210–221.
</div>
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <a href="https://doi.org/10.1007/978-1-0716-1418-1">An introduction to statistical learning</a>. Springer.
</div>
<div id="ref-jin2020advancing" class="csl-entry" role="listitem">
Jin, Y., B. Chen, B. D. Lampinen, and P. H. Brown. 2020. <a href="https://doi.org/10.3389/fpls.2020.00290">Advancing agricultural production with machine learning analytics: Yield determinants for california’s almond orchards</a>. Frontiers in Plant Science 11:290.
</div>
<div id="ref-jordan2015machine" class="csl-entry" role="listitem">
Jordan, M. I., and T. M. Mitchell. 2015. <a href="https://doi.org/10.1126/science.aaa8415">Machine learning: Trends, perspectives, and prospects</a>. Science 349:255–260.
</div>
<div id="ref-jumper2021highly" class="csl-entry" role="listitem">
Jumper, J., R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žı́dek, A. Potapenko, and others. 2021. <a href="https://doi.org/10.1038/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>. Nature 596:583–589.
</div>
<div id="ref-legendre1806nouvelles" class="csl-entry" role="listitem">
Legendre, A. M. 1806. Nouvelles m<span>é</span>thodes pour la d<span>é</span>termination des orbites des com<span>è</span>tes; par AM legendre... chez Firmin Didot, libraire pour lew mathematiques, la marine, l&nbsp;<span>…</span>.
</div>
<div id="ref-liu2022partial" class="csl-entry" role="listitem">
Liu, C., X. Zhang, T. T. Nguyen, J. Liu, T. Wu, E. Lee, and X. M. Tu. 2022. <a href="https://doi.org/10.1136/gpsych-2021-100662">Partial least squares regression and principal component analysis: Similarity and differences between two popular variable reduction approaches</a>. General Psychiatry 35.
</div>
<div id="ref-markov1912wahrscheinlichkeitsrechnung" class="csl-entry" role="listitem">
Markov, A. A. 1912. Wahrscheinlichkeitsrechnung. BG Teubner.
</div>
<div id="ref-mcculloch1943logical" class="csl-entry" role="listitem">
McCulloch, W. S., and W. Pitts. 1943. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics 5:115–133.
</div>
<div id="ref-mehmood2016diversity" class="csl-entry" role="listitem">
Mehmood, T., and B. Ahmed. 2016. <a href="https://doi.org/10.1002/cem.2762">The diversity in the applications of partial least squares: An overview</a>. Journal of Chemometrics 30:4–17.
</div>
<div id="ref-moghimi2020novel" class="csl-entry" role="listitem">
Moghimi, A., A. Pourreza, G. Zuniga-Ramirez, L. E. Williams, and M. W. Fidelibus. 2020. <a href="https://doi.org/10.3390/rs12213515">A novel machine learning approach to estimate grapevine leaf nitrogen concentration using aerial multispectral imagery</a>. Remote Sensing 12:3515.
</div>
<div id="ref-stockfish2023" class="csl-entry" role="listitem">
Noda, T. R. A. M. C. A. J. K. A. G. L. A. Y. N. A. M. I. A. H. 2023. Stockfish. https://github.com/official-stockfish/Stockfish; GitHub.
</div>
<div id="ref-omidi2020ensemble" class="csl-entry" role="listitem">
Omidi, R., A. Moghimi, A. Pourreza, M. El-Hadedy, and A. S. Eddin. 2020. <a href="https://doi.org/10.1109/ICMLA51294.2020.00054">Ensemble hyperspectral band selection for detecting nitrogen status in grape leaves</a>. IEEE.
</div>
<div id="ref-openai2023" class="csl-entry" role="listitem">
OpenAI. 2023. <a href="https://cdn.openai.com/papers/gpt-4.pdf">GPT-4 technical report</a>. OpenAI.
</div>
<div id="ref-plackett1949historical" class="csl-entry" role="listitem">
Plackett, R. L. 1949. <a href="https://doi.org/10.2307/2332682">A historical note on the method of least squares</a>. Biometrika 36:458–460.
</div>
<div id="ref-ravikanth2017extraction" class="csl-entry" role="listitem">
Ravikanth, L., D. S. Jayas, N. D. White, P. G. Fields, and D.-W. Sun. 2017. <a href="https://doi.org/10.1007/s11947-016-1817-8">Extraction of spectral information from hyperspectral data and application of hyperspectral imaging for food and agricultural products</a>. Food and bioprocess technology 10:1–33.
</div>
<div id="ref-rombach2021highresolution" class="csl-entry" role="listitem">
Rombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. 2021. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models</a>.
</div>
<div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, F. 1958. <a href="https://doi.org/10.1037/h0042519">The perceptron: A probabilistic model for information storage and organization in the brain.</a> Psychological review 65:386.
</div>
<div id="ref-rosipal2006overview" class="csl-entry" role="listitem">
Rosipal, R., and N. Krämer. 2006. <a href="https://doi.org/10.1007/11752790_2">Overview and recent advances in partial least squares</a>. Pages 34–51 Subspace, latent structure and feature selection: Statistical and optimization perspectives workshop, SLSFS 2005, bohinj, slovenia, february 23-25, 2005, revised selected papers. Springer.
</div>
<div id="ref-samuel1959machine" class="csl-entry" role="listitem">
Samuel, A. L. 1959. Machine learning. The Technology Review 62:42–45.
</div>
<div id="ref-silver2017mastering" class="csl-entry" role="listitem">
Silver, D., J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, and others. 2017. <a href="https://doi.org/10.1038/nature24270">Mastering the game of go without human knowledge</a>. Nature 550:354–359.
</div>
<div id="ref-takayama2016optimal" class="csl-entry" role="listitem">
Takayama, T., and A. Iwasaki. 2016. <a href="https://doi.org/10.5194/isprsannals-III-8-101-2016">Optimal wavelength selection on hyperspectral data with fused lasso for biomass estimation of tropical rainforests</a>. ISPRS Annals of Photogrammetry, Remote Sensing &amp; Spatial Information Sciences 3.
</div>
<div id="ref-tibshirani1996" class="csl-entry" role="listitem">
Tibshirani, R. 1996. <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">Regression shrinkage and selection via the lasso</a>. Journal of the Royal Statistical Society. Series B (Methodological) 58:267–288.
</div>
<div id="ref-van2018occam" class="csl-entry" role="listitem">
Van den Berg, H. 2018. <a href="https://doi.org/10.3184/003685018X15295002645082">Occam’s razor: From ockham’s via moderna to modern data science</a>. Science progress 101:261–272.
</div>
<div id="ref-verger2011optimal" class="csl-entry" role="listitem">
Verger, A., F. Baret, and F. Camacho. 2011. <a href="https://doi.org/10.1016/j.rse.2010.09.012">Optimal modalities for radiative transfer-neural network estimation of canopy biophysical characteristics: Evaluation over an agricultural area with CHRIS/PROBA observations</a>. Remote Sensing of Environment 115:415–426.
</div>
<div id="ref-vinyals2019grandmaster" class="csl-entry" role="listitem">
Vinyals, O., I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, and others. 2019. <a href="https://doi.org/10.1038/s41586-019-1724-z">Grandmaster level in StarCraft II using multi-agent reinforcement learning</a>. Nature 575:350–354.
</div>
<div id="ref-wang2017non" class="csl-entry" role="listitem">
Wang, J., C. Shen, N. Liu, X. Jin, X. Fan, C. Dong, and Y. Xu. 2017. <a href="https://doi.org/10.3390/s17030538">Non-destructive evaluation of the leaf nitrogen concentration by in-field visible/near-infrared spectroscopy in pear orchards</a>. Sensors 17:538.
</div>
<div id="ref-wold1984collinearity" class="csl-entry" role="listitem">
Wold, S., A. Ruhe, H. Wold, and W. Dunn Iii. 1984. <a href="https://doi.org/10.1137/0905052">The collinearity problem in linear regression. The partial least squares (PLS) approach to generalized inverses</a>. SIAM Journal on Scientific and Statistical Computing 5:735–743.
</div>
<div id="ref-yan2009linear" class="csl-entry" role="listitem">
Yan, X., and X. Su. 2009. <a href="https://doi.org/10.1142/6986">Linear regression analysis: Theory and computing</a>. world scientific.
</div>
<div id="ref-zou2005regularization" class="csl-entry" role="listitem">
Zou, H., and T. Hastie. 2005. <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">Regularization and variable selection via the elastic net</a>. Journal of the royal statistical society: series B (statistical methodology) 67:301–320.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./vegetation-indices-based-methods.html" class="pagination-link" aria-label="Vegetation-indices-based methods">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vegetation-indices-based methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./radiative-transfer-models.html" class="pagination-link" aria-label="Radiative transfer models">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Radiative transfer models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2023, Damian Oswald</p>
</div>   
    <div class="nav-footer-center">
<p>Remote Sensing of Foliar Nitrogen in Californian Almonds (2023)</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/damian-oswald/master-thesis/blob/main/machine-learning-methods-for-hyperspectral-data-analysis.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/damian-oswald/master-thesis/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Damian-Oswald">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/damian-oswald">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:damian.oswald@protonmail.com">
      <i class="bi bi-envelope-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>